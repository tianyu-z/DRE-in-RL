{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is developed based on https://github.com/seungeunrho/minimalRL\n",
    "import gym\n",
    "import random\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "def init_weights(net, init_type=\"normal\", init_gain=0.02, debug=False):\n",
    "    \"\"\"Initialize network weights.\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "    from torch.nn import init\n",
    "\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, \"weight\") and (\n",
    "            classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1\n",
    "        ):\n",
    "            if debug:\n",
    "                print(classname)\n",
    "            if init_type == \"normal\":\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == \"xavier\":\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == \"kaiming\":\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_in\")\n",
    "            elif init_type == \"orthogonal\":\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    \"initialization method [%s] is not implemented\" % init_type\n",
    "                )\n",
    "            if hasattr(m, \"bias\") and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif (\n",
    "            classname.find(\"BatchNorm2d\") != -1\n",
    "        ):  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    The Buffer for the off-policy training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, seq_data):\n",
    "        self.buffer.append(seq_data)\n",
    "\n",
    "    def sample(self, on_policy=False):\n",
    "        if on_policy:\n",
    "            mini_batch = [self.buffer[-1]]\n",
    "        else:\n",
    "            mini_batch = random.sample(self.buffer, batch_size)\n",
    "        s_lst, a_lst, r_lst, prob_lst, done_lst, is_first_lst = [], [], [], [], [], []\n",
    "        for seq in mini_batch:\n",
    "            is_first = True  # Flag for indicating whether the transition is the first item from a sequence\n",
    "            for transition in seq:\n",
    "                s, a, r, prob, done = transition\n",
    "                s_lst.append(s)\n",
    "                a_lst.append([a])\n",
    "                r_lst.append(r)\n",
    "                prob_lst.append(prob)\n",
    "                done_mask = 0.0 if done else 1.0\n",
    "                done_lst.append(done_mask)\n",
    "                is_first_lst.append(is_first)\n",
    "                is_first = False\n",
    "        s, a, r, prob, done_mask, is_first = (\n",
    "            torch.tensor(s_lst, dtype=torch.float),\n",
    "            torch.tensor(a_lst),\n",
    "            r_lst,\n",
    "            torch.tensor(prob_lst, dtype=torch.float),\n",
    "            done_lst,\n",
    "            is_first_lst,\n",
    "        )\n",
    "        return s, a, r, prob, done_mask, is_first\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The lstm domain classifer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, lstm_input_dim, lstm_num_layer, lstm_hidden_dim, lstm_output_dim\n",
    "    ):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            batch_first=True,\n",
    "            num_layers=lstm_num_layer,\n",
    "        )\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, lstm_output_dim)\n",
    "        self.lstm_input_dim = lstm_input_dim\n",
    "        self.lstm_num_layer = lstm_num_layer\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.lstm_output_dim = lstm_output_dim\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.lstm_num_layer, batch_size, self.lstm_hidden_dim),\n",
    "            torch.zeros(self.lstm_num_layer, batch_size, self.lstm_hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X, self.hidden = self.lstm(X)\n",
    "        X = self.fc(X)\n",
    "        X = self.sigmoid(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc_pi = nn.Linear(256, 2)\n",
    "        self.fc_q = nn.Linear(256, 2)\n",
    "\n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        pi = F.softmax(x, dim=softmax_dim)\n",
    "        return pi\n",
    "\n",
    "    def q(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        q = self.fc_q(x)\n",
    "        return q\n",
    "\n",
    "\n",
    "def train(model, optimizer, memory, on_policy=False, lstm=None):\n",
    "    s, a, r, prob, done_mask, is_first = memory.sample(on_policy)\n",
    "    if lstm is not None:\n",
    "        # prepare the data to create the ratio\n",
    "        tensor_list = []\n",
    "        for i in range(len(a)):\n",
    "            s0 = s[i].clone().cpu().numpy()\n",
    "            s0 = np.append(s0, a[i])\n",
    "            s0 = np.append(s0, r[i])\n",
    "            s0 = np.append(s0, prob[i])\n",
    "            tensor_list.append(torch.tensor(s0))\n",
    "        tensor_input = torch.vstack(tensor_list).unsqueeze(1)\n",
    "        prediction = lstm(tensor_input)\n",
    "        dynamic_ratio = prediction / (1 - prediction)\n",
    "        dynamic_ratio = dynamic_ratio.detach()\n",
    "        dynamic_ratio = dynamic_ratio[:, :, 0].repeat(1, 2)\n",
    "    else:\n",
    "        dynamic_ratio = 1\n",
    "    q = model.q(s)\n",
    "    q_a = q.gather(1, a)\n",
    "    pi = model.pi(s, softmax_dim=1)\n",
    "    pi_a = pi.gather(1, a)\n",
    "    v = (q * pi).sum(1).unsqueeze(1).detach()\n",
    "    rho = dynamic_ratio * pi.detach() / prob\n",
    "    rho_a = rho.gather(1, a)\n",
    "    rho_bar = rho_a.clamp(max=c)\n",
    "    correction_coeff = (1 - c / rho).clamp(min=0)\n",
    "    q_ret = v[-1] * done_mask[-1]\n",
    "    q_ret_lst = []\n",
    "    for i in reversed(range(len(r))):\n",
    "        q_ret = r[i] + gamma * q_ret\n",
    "        q_ret_lst.append(q_ret.item())\n",
    "        q_ret = rho_bar[i] * (q_ret - q_a[i]) + v[i]\n",
    "        if is_first[i] and i != 0:\n",
    "            q_ret = (\n",
    "                v[i - 1] * done_mask[i - 1]\n",
    "            )  # When a new sequence begins, q_ret is initialized\n",
    "    q_ret_lst.reverse()\n",
    "    q_ret = torch.tensor(q_ret_lst, dtype=torch.float).unsqueeze(1)\n",
    "    loss1 = -rho_bar * torch.log(pi_a) * (q_ret - v)\n",
    "    loss2 = (\n",
    "        -correction_coeff * pi.detach() * torch.log(pi) * (q.detach() - v)\n",
    "    )  # bias correction term\n",
    "    loss = loss1 + loss2.sum(1) + F.smooth_l1_loss(q_a, q_ret)\n",
    "    optimizer.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristics\n",
    "# 1. Discrete action space, single thread version.\n",
    "# 2. Does not support trust-region updates.\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma = 0.98\n",
    "buffer_limit = 6000\n",
    "rollout_len = 10\n",
    "batch_size = (\n",
    "    4  # Indicates 4 sequences per mini-batch (4*rollout_len = 40 samples total)\n",
    ")\n",
    "c = 1.0  # For truncating importance sampling ratio\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "# initialize models incide FFNN for the ActorCritic and lstm for the domain classifier\n",
    "model = ActorCritic()\n",
    "lstm = LSTM(8, 5, 128, 1).double()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer_lstm = optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "init_weights(model, init_type=\"xavier\")\n",
    "init_weights(lstm, init_type=\"xavier\")\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "score = 0.0\n",
    "print_interval = 1\n",
    "save_interval = 100\n",
    "max_epi = 10000\n",
    "\n",
    "# saving checkpoints\n",
    "save_dict = {}\n",
    "save_dict[\"n_epi\"] = []\n",
    "save_dict[\"score\"] = []\n",
    "\n",
    "for n_epi in range(max_epi):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # train the lstm domain classifier after some episode because we need to collect some tensor data for training\n",
    "    if n_epi > 100:\n",
    "        print(\"Training the lstm classifier.\")\n",
    "        optimizer_lstm.zero_grad()\n",
    "        min_len = min([len(tensor_data_old), len(tensor_data)])\n",
    "        tensor_data_old_train = tensor_data_old[:min_len]\n",
    "        tensor_data = tensor_data[:min_len]\n",
    "        label = [[0] * min_len + [1] * min_len]\n",
    "        if not isinstance(tensor_data_old_train, torch.Tensor):\n",
    "            tensor_data_old_train = torch.vstack((tensor_data_old_train))\n",
    "        if not isinstance(tensor_data, torch.Tensor):\n",
    "            tensor_data = torch.vstack((tensor_data))\n",
    "        input_data = (\n",
    "            torch.vstack((tensor_data_old_train, tensor_data)).unsqueeze(0).double()\n",
    "        )\n",
    "        input_label = torch.tensor(label).unsqueeze(-1).double()\n",
    "        prediction = lstm(input_data)\n",
    "        loss = mse_loss(prediction, input_label)\n",
    "        loss.mean().backward()\n",
    "        optimizer_lstm.step()\n",
    "\n",
    "    # initialize tensor data buffer\n",
    "    if n_epi == 0:\n",
    "        tensor_data_old = []\n",
    "        tensor_data = []\n",
    "    # update tensor data buffer\n",
    "    else:\n",
    "        tensor_data_old = tensor_data\n",
    "        tensor_data = []\n",
    "\n",
    "    while not done:\n",
    "        seq_data = []\n",
    "        for t in range(rollout_len):\n",
    "            prob = model.pi(torch.from_numpy(s).float())\n",
    "            a = Categorical(prob).sample().item()\n",
    "            s_prime, r, done, info = env.step(a)\n",
    "            prob_numpy = prob.detach().numpy()\n",
    "            seq_data.append((s, a, r / 100.0, prob_numpy, done))\n",
    "\n",
    "            # collect input tensor for lstm\n",
    "            s0 = s.copy()\n",
    "            s0 = np.append(s0, a)\n",
    "            s0 = np.append(s0, r / 100)\n",
    "            s0 = np.append(s0, prob_numpy)\n",
    "            tensor_data.append(torch.tensor(s0))\n",
    "\n",
    "            score += r\n",
    "            s = s_prime\n",
    "            if done:\n",
    "                break\n",
    "        memory.put(seq_data)\n",
    "        # if episode<=100, train without lstm\n",
    "        if memory.size() > 500 and n_epi <= 100:\n",
    "            train(model, optimizer, memory)\n",
    "        # if episode>100, train with the lstm as the domain classifer\n",
    "        elif memory.size() > 500 and n_epi >= 100:\n",
    "            train(model, optimizer, memory, lstm=lstm)\n",
    "    if n_epi % print_interval == 0 and n_epi != 0:\n",
    "        print(\n",
    "            \"# of episode :{}, avg score : {:.1f}, buffer size : {}\".format(\n",
    "                n_epi, score / print_interval, memory.size()\n",
    "            )\n",
    "        )\n",
    "        save_dict[\"n_epi\"].append(n_epi)\n",
    "        save_dict[\"score\"].append(score)\n",
    "        if n_epi % save_interval == 0 and n_epi != 0:\n",
    "            save_dict[\"model\"] = model.cpu().state_dict()\n",
    "            save_dict[\"lstm\"] = lstm.cpu().state_dict()\n",
    "            torch.save(save_dict, \"ckpt_{}_with_lstm.pth\".format(n_epi))\n",
    "        score = 0.0\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
